目前做常识生成的几个角度和切入点：

1. 生成框架

   > EKI-BART：对BART的cross-attention魔改，用的检索框架
   >
   > KFCNet：在encoder和decoder端加入了对比学习

2. 数据增广

  > RE-T5：加强检索框架，没做模型修改
  > EKI-BART：检索框架

3. 解码端算法
   
   > neurologic decoding: 在解码端，基于beam search魔改使用constrained decoding方法。
   
4. 目前考虑生成端，也就是encoder，或者说输入的这些词袋，能做什么处理，之前看到目前比较火的prompt，想过在这上面魔改，但目前还没什么好的想法。

5. embedding transfer

   这个东西研究的很多，在这个任务上或许能提高性能，但是很难说能不能成为一个独立的idea.

继续看文章QAQ